{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>AI Report William Forber:22015706</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "from keras import layers\n",
    "from baseline_model import base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataset And setting the redundant, category [boolean], and numerical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../Dataset/Synthetic_Data_For_Students.csv')\n",
    "\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "redundant_labels = ['Accident Description', 'Injury Description', 'Claim Date', 'Accident Date',\n",
    "                    'SpecialHealthExpenses', 'SpecialReduction', 'SpecialOverage', 'GeneralRest',\n",
    "                    'SpecialAdditionalInjury', 'SpecialEarningsLoss', 'SpecialUsageLoss', 'SpecialMedications',\n",
    "                    'SpecialAssetDamage', 'SpecialRehabilitation', 'SpecialFixes', 'GeneralFixed', 'GeneralUplift',\n",
    "                    'SpecialLoanerVehicle', 'SpecialTripCosts', 'SpecialJourneyExpenses', 'SpecialTherapy']\n",
    "\n",
    "category_labels = ['AccidentType', 'Exceptional_Circumstances', 'Minor_Psychological_Injury', 'Dominant injury',\n",
    "                   'Whiplash', 'Vehicle Type', 'Weather Conditions',\n",
    "                   'Police Report Filed', 'Witness Present', 'Gender']\n",
    "\n",
    "numerical_labels = ['SettlementValue', 'Injury_Prognosis',\n",
    "                    'Vehicle Age', 'Driver Age', 'Number of Passengers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Defining functions for cleaning the dataset, and categorising the data, which also scales the data using sklearns standard scaler, alo defining a build model function to build a tensorflow nerual network with different hyper parameters and an evaluate funciton which uses mean absolute error to rate models. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def clean_dataset(data):\n",
    "    data.dropna(inplace=True)\n",
    "    data.drop(redundant_labels, axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def categorise_data(data, label):\n",
    "    categories = pd.get_dummies(data[label])\n",
    "    data.drop(label, axis=1, inplace=True)\n",
    "    data = pd.concat([data, categories], axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_months(prognosis):\n",
    "    return int(''.join(filter(str.isdigit, prognosis)))\n",
    "\n",
    "\n",
    "def scale_data(data):\n",
    "    scaler = StandardScaler()\n",
    "    data[numerical_labels] = scaler.fit_transform(data[numerical_labels])\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data = clean_dataset(data)\n",
    "    data['Injury_Prognosis'] = data['Injury_Prognosis'].apply(extract_months)\n",
    "    data = scale_data(data)\n",
    "    for label in category_labels:\n",
    "        data = categorise_data(data, label)\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_model(hyper_parameters):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(hyper_parameters.Int('units_1', min_value=32, max_value=256, step=32),\n",
    "                           activation=hyper_parameters.Choice(\n",
    "                               'activation_1', ['relu', 'tanh', 'leaky_relu']),\n",
    "                           input_shape=(X_train_tf.shape[1],)))\n",
    "\n",
    "    for i in range(hyper_parameters.Int('num_layers', 1, 3)):\n",
    "        model.add(layers.Dense(hyper_parameters.Int(f'units_{i+2}', min_value=32, max_value=256, step=32),\n",
    "                               activation=hyper_parameters.Choice(f'activation_{i+2}', ['relu', 'tanh', 'leaky_relu'])))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=hyper_parameters.Choice('learning_rate', [0.01, 0.001, 0.0001])),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model):\n",
    "    test_loss, test_mae = model.evaluate(X_test_tf, y_test_tf)\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    print(f'Test MAE (Mean Absolute Error): {test_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Splitting the dataset into train and test data and converting them to tensorflow data for use with amd rocm</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = preprocess_data(dataset)\n",
    "\n",
    "X = dataset.drop('SettlementValue', axis=1)\n",
    "y = dataset['SettlementValue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "X_train_np = X_train.values\n",
    "X_test_np = X_test.values\n",
    "y_train_np = y_train.values\n",
    "y_test_np = y_test.values\n",
    "\n",
    "X_train_tf = tf.convert_to_tensor(X_train_np, dtype=tf.float32)\n",
    "X_test_tf = tf.convert_to_tensor(X_test_np, dtype=tf.float32)\n",
    "y_train_tf = tf.convert_to_tensor(y_train_np, dtype=tf.float32)\n",
    "y_test_tf = tf.convert_to_tensor(y_test_np, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a base model with no hyper parameters and evaluating it to compare it to the tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def base_model(X_train_tf):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(\n",
    "            X_train_tf.shape[1],)),  # Input layer\n",
    "        layers.Dense(64, activation='relu'),  # Hidden layer\n",
    "        layers.Dense(32, activation='relu'),  # Hidden layer\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "base_model = base_model(X_train_tf)\n",
    "base_model.fit(X_train_tf, y_train_tf, epochs=50,\n",
    "               batch_size=32, validation_data=(X_test_tf, y_test_tf))\n",
    "\n",
    "print(\"Base model results: No hyper parameter tuning:\")\n",
    "evaluate_model(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using sklearns random search to find the best combination of hyper parmeters using mean absoluate error and running it on a new model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mae',\n",
    "    max_trials=15,\n",
    "    executions_per_trial=2,\n",
    "    directory='tuner_results',\n",
    "    project_name='nerual_network_test'\n",
    ")\n",
    "\n",
    "tuner.search(X_train_tf, y_train_tf, epochs=50, batch_size=32,\n",
    "             validation_data=(X_test_tf, y_test_tf))\n",
    "\n",
    "best_hyper_parameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "tuned_model = tuner.hypermodel.build(best_hyper_parameters)\n",
    "\n",
    "history = tuned_model.fit(X_train_tf, y_train_tf, epochs=50,\n",
    "                          batch_size=32, validation_data=(X_test_tf, y_test_tf))\n",
    "\n",
    "print(\"tuned model results: hyper parameters tuned:\")\n",
    "evaluate_model(tuned_model)\n",
    "\n",
    "print(f'Best Hyper Parameters \\n {best_hyper_parameters.get_config()}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
